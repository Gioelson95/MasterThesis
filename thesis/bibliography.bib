@inproceedings{feng2003popular,
abstract = {In the community of music information retrieval, researchers developed methods to retrieval music with a particular melody, they also developed methods to retrieval music by similarity. We aim to retrieval music by mood, which is sometimes the exclusive manner people select music to enjoy, for example, when someone is sad for some reason, she or he wants to listen to a piece of music that can cheer her or him up, at this moment she or he will search music segment by mood no matter what the melody sounds and which the piece of music is similar to. In the essence, the difficulty for music retrieval by mood stems from the gap between the rich meanings that users want when they query and the shallowness of content descriptor we can actually compute.},
address = {Hangzhou, China},
author = {Feng, Yazhong and Zhuang, Yueting and Pan, Yunhe},
title = {{Popular music retrieval by detecting mood}},
booktitle={Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},
pages = {375--376},
year = {2003}
}
@book{yang2011music,
  title={Music emotion recognition},
  author={Yang, Yi-Hsuan and Chen, Homer H},
  year={2011},
  publisher={CRC Press, Inc.},
  address = {USA},
  edition = {1st} ,
  isbn = {9781439850466}
}
@inproceedings{lee2004survey,
  title={Survey of music information needs, uses, and seeking behaviours: preliminary findings.},
  author={Lee, Jin Ha and Downie, J Stephen},
  booktitle={ISMIR},
  volume={2004},
  pages={5th},
  year={2004},
  organization={Citeseer}
}
@article{juslin2004expression,
  title={Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening},
  author={Juslin, Patrik N and Laukka, Petri},
  journal={Journal of new music research},
  volume={33},
  number={3},
  pages={217--238},
  year={2004},
  publisher={Taylor \& Francis}
}
@inproceedings{van2006emotion,
  title={Emotion detection in music, a survey},
  author={Van De Laar, Bram},
  booktitle={Twente Student Conference on IT},
  volume={1},
  pages={700},
  year={2006}
}
@article{hevner1935expression,
  title={Expression in music: a discussion of experimental studies and theories.},
  author={Hevner, Kate},
  journal={Psychological review},
  volume={42},
  number={2},
  pages={186},
  year={1935},
  publisher={Psychological Review Company}
}
@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}
@article{macdorman2007automatic,
  title={Automatic emotion prediction of song excerpts: Index construction, algorithm design, and empirical comparison},
  author={MacDorman, Stuart Ough Chin-Chang Ho, Karl F},
  journal={Journal of New Music Research},
  volume={36},
  number={4},
  pages={281--299},
  year={2007},
  publisher={Taylor \& Francis}
}
@article{kensinger2004remembering,
  title={Remembering emotional experiences: The contribution of valence and arousal},
  author={Kensinger, Elizabeth A},
  journal={Reviews in the Neurosciences},
  volume={15},
  number={4},
  pages={241--252},
  year={2004},
  publisher={De Gruyter}
}
@article{gabrielsson2001influence,
  title={The influence of musical structure on emotional expression.},
  author={Gabrielsson, Alf and Lindstr{\"o}m, Erik},
  year={2001},
  publisher={Oxford University Press}
}
@article{yang2018review,
  title={Review of data features-based music emotion recognition methods},
  author={Yang, Xinyu and Dong, Yizhuo and Li, Juan},
  journal={Multimedia Systems},
  volume={24},
  number={4},
  pages={365--389},
  year={2018},
  publisher={Springer}
}
@article{zhang2017feature,
  title={Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods},
  author={Zhang, Jiang Long and Huang, Xiang Lin and Yang, Li Fang and Xu, Ye and Sun, Shu Tao},
  journal={Multimedia Systems},
  volume={23},
  number={2},
  pages={251--264},
  year={2017},
  publisher={Springer}
}
@article{robnik2003theoretical,
  title={Theoretical and empirical analysis of ReliefF and RReliefF},
  author={Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
  journal={Machine learning},
  volume={53},
  number={1-2},
  pages={23--69},
  year={2003},
  publisher={Springer}
}
@article{panda2018novel,
  title={Novel audio features for music emotion recognition},
  author={Panda, Renato and Malheiro, Ricardo Manuel and Paiva, Rui Pedro},
  journal={IEEE Transactions on Affective Computing},
  year={2018},
  publisher={IEEE}
}
@book{geron2019hands,
  title={Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
  author={G{\'e}ron, Aur{\'e}lien},
  year={2019},
  publisher={O'Reilly Media}
}








@article{Pulkki2007,
abstract = {Directional audio coding (DirAC) is a method for spatial sound representation, applicable for different sound reproduction systems. In the analysis part the diffuseness and direction of arrival of sound are estimated in a single location depending on time and frequency. In the synthesis part microphone signals are first divided into nondiffuse and diffuse parts, and are then reproduced using different strategies. DirAC is developed from an existing technology for impulse response reproduction, spatial impulse response rendering (SIRR), and imple- mentations of DirAC for different applications are described.},
author = {Pulkki, Ville},
file = {:media/francesco/Data/Mega/Mendeley/Pulkki - 2007 - Spatial sound reproduction with directional audio coding.pdf:pdf},
issn = {0004-7554},
journal = {J. Audio Eng. Soc.},
number = {6},
pages = {503--516},
pmid = {34447274778},
title = {{Spatial sound reproduction with directional audio coding}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=14170},
volume = {55},
year = {2007}
}